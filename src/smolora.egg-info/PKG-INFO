Metadata-Version: 2.4
Name: smolora
Version: 0.1.0
Summary: A lightweight, developer-friendly Python tool for fine-tuning small language models using LoRA adapters.
Author: SmoLoRA Contributors
License: MIT
Project-URL: Homepage, https://github.com/username/smolora
Project-URL: Bug Reports, https://github.com/username/smolora/issues
Project-URL: Source, https://github.com/username/smolora
Keywords: machine-learning,deep-learning,transformers,lora,fine-tuning
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.20.0
Requires-Dist: datasets>=2.0.0
Requires-Dist: peft>=0.4.0
Requires-Dist: trl>=0.7.0
Requires-Dist: accelerate>=0.20.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Dynamic: license-file

![SmoLoRA Logo](logo.png)

# SmoLoRA: Edge Language Model Fine-Tuning & Inference Toolkit

A lightweight, developer-friendly Python package for fine-tuning small language models using LoRA adapters and running on-device inference. Built for flexibility and rapid prototyping, SmoLoRA allows you to train, save, load, and generate text from language models with a clean, modular architecture.

---

## Table of Contents
- [ğŸ“¦ Features](#-features)
- [ğŸ”§ Installation](#-installation)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“‚ Custom Dataset Handling](#-custom-dataset-handling)
- [ğŸ› ï¸ Advanced Usage](#-advanced-usage)
- [ğŸ§  Tips & Best Practices](#-tips--best-practices)
- [ğŸ§ª Testing](#-testing)
- [ğŸ“š Documentation](#-documentation)

---

## ğŸ“¦ Features

- **Easy Installation**: Simple pip install with all dependencies managed
- **Modular Architecture**: Clean separation between core functionality and utilities
- **Multiple Data Sources**: Support for HuggingFace datasets, local text files, CSV, and JSONL
- **LoRA Fine-tuning**: Efficient fine-tuning using PEFT LoRA adapters
- **Model Management**: Save, load, and merge adapters with base models
- **Comprehensive Testing**: Full test suite with mocking for reliable development
- **Developer Tools**: Pre-commit hooks, formatting, and linting included

---

## ğŸ”§ Installation

### Quick Install
```bash
pip install smolora
```

### Development Setup
```bash
# Clone the repository
git clone https://github.com/username/smolora.git
cd smolora

# Run the development setup script
chmod +x scripts/setup-dev.sh
./scripts/setup-dev.sh
```

This will create a virtual environment, install all dependencies, and set up pre-commit hooks.

---

## ğŸ“ Project Structure

```
smolora/
â”œâ”€â”€ src/smolora/           # Main package source
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ core.py            # Main SmoLoRA class
â”‚   â””â”€â”€ dataset.py         # Dataset handling utilities
â”œâ”€â”€ examples/              # Usage examples
â”‚   â””â”€â”€ usage.py           # Basic usage example
â”œâ”€â”€ tests/                 # Test suite
â”‚   â””â”€â”€ test_smolora.py    # Comprehensive tests
â”œâ”€â”€ scripts/               # Development scripts
â”‚   â””â”€â”€ setup-dev.sh       # Development environment setup
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ api-reference.md   # API documentation
â”‚   â”œâ”€â”€ architecture.md    # Architecture overview
â”‚   â””â”€â”€ ...               # Additional documentation
â”œâ”€â”€ pyproject.toml         # Project configuration
â”œâ”€â”€ requirements.txt       # Production dependencies
â”œâ”€â”€ dev-requirements.txt   # Development dependencies
â””â”€â”€ README.md             # This file
---

## ğŸš€ Quick Start

### Basic Usage

```python
from smolora import SmoLoRA

# Initialize the trainer
trainer = SmoLoRA(
    base_model_name="microsoft/Phi-1.5",
    dataset_name="yelp_review_full",
    text_field="text",
    output_dir="./output_model"
)

# Fine-tune the model
trainer.train()

# Save the adapter and merge with base model
trainer.save()

# Load the merged model for inference
model, tokenizer = trainer.load_model("./output_model/final_merged")

# Generate text
prompt = "Write a review about a great coffee shop."
result = trainer.inference(prompt)
print("Generated output:", result)
```

### Using Custom Datasets

```python
from smolora import SmoLoRA, load_text_data, prepare_dataset

# Load data from local text files
dataset = load_text_data("./my_text_files/")

# Or prepare data from various formats
dataset = prepare_dataset(
    source="./data/my_data.jsonl",
    text_field="content",
    chunk_size=50  # Optional: split into chunks
)

# Use with SmoLoRA
trainer = SmoLoRA(
    base_model_name="microsoft/Phi-1.5",
    dataset_name=dataset,  # Use the prepared dataset directly
    output_dir="./custom_model"
)
```

---

## ğŸ“‚ Custom Dataset Handling

SmoLoRA supports multiple data formats through the `smolora.dataset` module:

### Text Files
```python
from smolora.dataset import load_text_data

# Load all .txt files from a directory
dataset = load_text_data("./text_directory/")
```

### JSONL Files
```python
from smolora.dataset import prepare_dataset

# Prepare JSONL data
dataset = prepare_dataset(
    source="data.jsonl",
    text_field="text",  # Field containing the text data
    chunk_size=100      # Optional: words per chunk
)
```

### CSV Files
```python
from smolora.dataset import prepare_dataset

# Prepare CSV data
dataset = prepare_dataset(
    source="data.csv",
    text_field="content",
    file_type="csv"  # Explicitly specify format
)
```

If you want to fine-tune on your own text files, use the helper function from `local_text.py` to load your data:

```python
from datasets import Dataset
from local_text import load_text_data

# Load text data from a local folder (each .txt file can contain one or multiple text entries).
dataset = load_text_data("./my_text_data")

# Pass the dataset into the trainer by replacing the dataset name.
from LoRATrainer import LoRATrainer

trainer = LoRATrainer(
    base_model_name="meta-llama/Llama-2-7b-hf",
    dataset_name="yelp_review_full",  # placeholder: change as needed if supporting custom datasets
    text_field="text",
    output_dir="./custom_text_output"
)

# For custom in-memory datasets, adjust the trainer's dataset member as needed:
trainer.dataset = dataset

# Proceed with training, merging, loading, and inference as shown above.
```

---

## ğŸ› ï¸ General-Purpose Dataset Preparation

You can use the `prepare_dataset.py` tool to convert your raw text, CSV, or JSONL data into a HuggingFace `Dataset` ready for fine-tuning.

---

## ğŸ› ï¸ Advanced Usage

### Configuration Options

The `SmoLoRA` class accepts several parameters for customization:

```python
trainer = SmoLoRA(
    base_model_name="microsoft/Phi-1.5",  # Any HuggingFace model
    dataset_name="yelp_review_full",      # HF dataset or custom Dataset object
    text_field="text",                    # Field containing text data
    output_dir="./fine_tuned_model"       # Output directory
)
```

### LoRA Configuration

You can customize the LoRA adapter settings by modifying the `peft_config` after initialization:

```python
trainer = SmoLoRA(...)
trainer.peft_config.r = 16              # Rank
trainer.peft_config.lora_alpha = 32     # Alpha scaling
trainer.peft_config.lora_dropout = 0.1  # Dropout
```

### Training Configuration

Customize training parameters through the trainer's configuration:

```python
# Access training configuration
training_config = trainer.training_args
training_config.num_train_epochs = 3
training_config.per_device_train_batch_size = 2
training_config.learning_rate = 2e-4
```

---

## ğŸ§  Tips & Best Practices

- **Start Small**: Begin with smaller models like Phi-1.5 for faster iteration
- **Memory Management**: Use gradient checkpointing for larger models
- **Data Quality**: Clean and consistent training data leads to better results
- **Evaluation**: Monitor training loss and validate on held-out data
- **Chunking**: Use appropriate chunk sizes for your specific use case
- **Device Selection**: The toolkit automatically uses MPS on Apple Silicon Macs

---

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src/smolora --cov-report=html

# Run specific test categories
pytest tests/ -m unit        # Unit tests only
pytest tests/ -m integration # Integration tests only
```

The test suite includes:
- Unit tests for core functionality
- Dataset loading and preparation tests
- Mock-based training pipeline tests
- Integration tests with sample data

---

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[API Reference](docs/api-reference.md)**: Complete API documentation
- **[Architecture](docs/architecture.md)**: System design and components
- **[Dataset Handling](docs/dataset-handling.md)**: Data preparation guide
- **[Development Guide](docs/development-guide.md)**: Contributing guidelines

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following the coding standards
4. Run tests and ensure they pass
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/username/smolora.git
cd smolora
chmod +x scripts/setup-dev.sh
./scripts/setup-dev.sh
```

This sets up pre-commit hooks, installs development dependencies, and configures the environment.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built on top of HuggingFace Transformers, PEFT, and TRL
- Inspired by the need for accessible fine-tuning tools
- Thanks to the open-source ML community

```python
from LoRATrainer import LoRATrainer
from local_text import load_text_data
from datetime import datetime

start = datetime.now()
print("Welcome to SmoLoRA!")
print("Initializing the trainer...")

# Choose your base model
base_model = "microsoft/Phi-1.5"
# Alternate: base_model = "meta-llama/Llama-2-7b-hf"

# Choose your dataset identifier
dataset = "yelp_review_full"
# Alternatively, use custom local text data:
# dataset = load_text_data("./my_text_data")
# Or use the general-purpose preparer:
# from prepare_dataset import prepare_dataset
# dataset = prepare_dataset("./my_texts_folder", chunk_size=128)

# Define a prompt for inference
prompt = "Write a review about a great coffee shop."

print("Initializing the trainer...")
trainer = LoRATrainer(
    base_model_name=base_model,
    dataset_name=dataset,
    text_field="text",
    output_dir="./output_model"
)

trainer_init_time = datetime.now()
print(f"Trainer initialized in {trainer_init_time - start}")

# Fine-tune the model.
print("Starting model tuning...")
trainer.train()
trainer_tune_time = datetime.now()
print(f"Model tuned in {trainer_tune_time - trainer_init_time}")

# Merge the LoRA adapter with the base model and save.
print("Merging the model and saving...")
trainer.save()
trainer_save_time = datetime.now()
print(f"Merged and saved in {trainer_save_time - trainer_tune_time}")

# Load the tuned (merged) model.
print("Loading the tuned model...")
model, tokenizer = trainer.load_model("./output_model/final_merged")
load_model_time = datetime.now()
print(f"Model loaded in {load_model_time - trainer_save_time}")

# Run a single inference.
print("Running inference...")
result = trainer.inference(prompt)
print("Generated output:", result)
inference_time = datetime.now()
print(f"Inference completed in {inference_time - load_model_time}")

print("Bye now!")
```

---

## ğŸ§  Tips & Best Practices

- For rapid prototyping, use a publicly available HuggingFace dataset such as `"yelp_review_full"`.
- When working with your custom text files, use the provided `load_text_data` function or the general-purpose `prepare_dataset.py` tool to load and format your data.
- After training, merging the adapter with the base model allows you to deploy a single, portable model.
- Adjust training parameters within the code if you need deeper control over hyperparameters.
- Use the timing logs in `usage.py` to evaluate performance during training and inference.

---

## ğŸ“‚ Files

```
slm-tuner/
â”œâ”€â”€ LoRATrainer.py          # Core tool implementation handling training & inference
â”œâ”€â”€ local_text.py           # Utility to load custom text files into a dataset
â”œâ”€â”€ prepare_dataset.py      # General-purpose dataset preparation tool (text, CSV, JSONL)
â”œâ”€â”€ usage.py                # Example script with timing logs for a full workflow
â””â”€â”€ README.md               # Project overview and usage instructions
```

---

Happy fine-tuning! ğŸ¦™ğŸ’»âœ¨
